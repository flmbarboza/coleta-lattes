import io
import re
import json
import time
import requests
import pandas as pd
import streamlit as st
import plotly.express as px
from PyPDF2 import PdfReader
from rapidfuzz import fuzz
from openai import OpenAI

# ---------------------------
# Configura√ß√£o Streamlit
# ---------------------------
st.set_page_config(page_title="Lattes PDF ‚Üí Artigos ‚Üí Gr√°ficos", layout="wide")
st.title("üìö Lattes (PDF) ‚Üí Artigos em Peri√≥dicos ‚Üí Confirma√ß√£o ‚Üí Gr√°ficos")

# ---------------------------
# Helpers: PDF -> texto
# ---------------------------
def pdf_to_text(file_bytes: bytes) -> str:
    reader = PdfReader(io.BytesIO(file_bytes))
    parts = []
    for p in reader.pages:
        parts.append(p.extract_text() or "")
    return "\n".join(parts)

# ---------------------------
# Localizar se√ß√£o de Artigos em Peri√≥dicos
# ---------------------------
def slice_journal_section(text: str) -> str:
    clean = re.sub(r"[ \t]+", " ", text)
    clean = re.sub(r"\n{2,}", "\n", clean)

    patterns_start = [
        r"Artigos completos publicados em peri√≥dicos",
        r"Artigos publicados em peri√≥dicos",
        r"Artigos completos em peri√≥dicos",
    ]
    patterns_end = [
        r"Trabalhos completos publicados em anais",
        r"Livros publicados",
        r"Cap√≠tulos de livros",
        r"Textos em jornais",
        r"Produ√ß√£o t√©cnica",
        r"Demais tipos de produ√ß√£o bibliogr√°fica"
    ]

    start = None
    for ps in patterns_start:
        m = re.search(ps, clean, flags=re.IGNORECASE)
        if m:
            start = m.start()
            break
    if start is None:
        return ""

    sub = clean[start:]
    end = None
    for pe in patterns_end:
        m = re.search(pe, sub, flags=re.IGNORECASE)
        if m:
            end = m.start()
            break
    if end is not None:
        sub = sub[:end]

    # remove o cabe√ßalho
    sub = re.sub(r"^.*?peri√≥dicos\s*\n", "", sub, flags=re.IGNORECASE | re.DOTALL)
    return sub.strip()

# ---------------------------
# Heur√≠stica r√°pida (fallback / pr√©via)
# ---------------------------
def extract_articles_heuristic(section: str) -> pd.DataFrame:
    if not section:
        return pd.DataFrame(columns=["ano", "titulo", "doi", "raw"])

    items = re.split(r"\n(?=\d+\.)", section)
    doi_regex = r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b"

    rows = []
    for it in items:
        it = it.strip()
        if len(it) < 30:
            continue

        mdoi = re.search(doi_regex, it, flags=re.IGNORECASE)
        doi = mdoi.group(0).rstrip(" .;,") if mdoi else ""

        years = re.findall(r"\b(19\d{2}|20\d{2})\b", it)
        ano = int(years[-1]) if years else None

        parts = [p.strip() for p in it.split(".") if p.strip()]
        titulo = parts[1] if len(parts) >= 2 else re.sub(r"^\d+\.\s*", "", it.split("\n")[0]).strip()

        rows.append({"ano": ano, "titulo": titulo, "doi": doi, "raw": it})

    df = pd.DataFrame(rows)
    if not df.empty:
        df["titulo"] = df["titulo"].fillna("").str.replace(r"\s+", " ", regex=True).str.strip()
        df["doi"] = df["doi"].fillna("").str.strip()
    return df

# ---------------------------
# LLM: extrair artigos estruturados (JSON)
# ---------------------------
def get_openai_client():
    api_key = st.secrets.get("OPENAI_API_KEY", "")
    if not api_key:
        return None
    return OpenAI(api_key=api_key)

def llm_extract_articles(section: str) -> pd.DataFrame:
    client = get_openai_client()
    if client is None:
        st.error("OPENAI_API_KEY n√£o configurada em st.secrets.")
        return pd.DataFrame(columns=["ano", "titulo", "doi"])

    model = st.secrets.get("OPENAI_MODEL", "gpt-4o-mini")

    # reduz tamanho para evitar prompt enorme
    section = section[:45000]

    system = (
        "Voc√™ √© um assistente que extrai produ√ß√£o bibliogr√°fica de Curr√≠culo Lattes (texto do PDF). "
        "Extraia SOMENTE 'Artigos completos publicados em peri√≥dicos'. "
        "Retorne JSON estrito com a chave 'articles', uma lista de objetos com: "
        "{'year': int, 'title': str, 'doi': str | null}. "
        "Se DOI n√£o existir, use null. N√£o invente dados."
    )

    resp = client.chat.completions.create(
        model=model,
        temperature=0,
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": f"Texto da se√ß√£o:\n\n{section}"}
        ],
    )

    content = resp.choices[0].message.content
    data = json.loads(content)

    articles = data.get("articles", [])
    rows = []
    for a in articles:
        rows.append({
            "ano": a.get("year"),
            "titulo": (a.get("title") or "").strip(),
            "doi": (a.get("doi") or "") if a.get("doi") is not None else ""
        })

    df = pd.DataFrame(rows)
    if df.empty:
        return pd.DataFrame(columns=["ano", "titulo", "doi"])
    df["ano"] = pd.to_numeric(df["ano"], errors="coerce").astype("Int64")
    df["titulo"] = df["titulo"].astype(str).str.replace(r"\s+", " ", regex=True).str.strip()
    df["doi"] = df["doi"].astype(str).str.strip()
    return df

# ---------------------------
# OpenAlex: cita√ß√µes por DOI (singleton por DOI)
# ---------------------------
OPENALEX_BASE = "https://api.openalex.org"

def openalex_work_by_doi(doi: str):
    """
    OpenAlex permite buscar work por DOI via:
    /works/https://doi.org/<DOI>  (external ID)
    """
    api_key = st.secrets.get("OPENALEX_API_KEY", "")
    if not api_key:
        return None

    url = f"{OPENALEX_BASE}/works/https://doi.org/{doi}"
    r = requests.get(url, params={"api_key": api_key}, timeout=20)
    if r.status_code == 200:
        return r.json()
    return None

def add_citations(df: pd.DataFrame, progress_cb=None) -> pd.DataFrame:
    if df.empty:
        df["citacoes"] = []
        return df

    citations = []
    for i, row in df.iterrows():
        doi = (row.get("doi") or "").strip()
        cited = None
        if doi:
            try:
                work = openalex_work_by_doi(doi)
                if work:
                    cited = work.get("cited_by_count", None)
            except Exception:
                cited = None

        citations.append(cited)
        if progress_cb:
            progress_cb(i + 1, len(df))
        time.sleep(0.12)

    out = df.copy()
    out["citacoes"] = citations
    return out

# ---------------------------
# Chat / Estado
# ---------------------------
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Ol√°! Fa√ßa upload do **PDF do Curr√≠culo Lattes**. Vou extrair os **artigos em peri√≥dicos**, pedir sua confirma√ß√£o e gerar os gr√°ficos."}
    ]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

uploaded = st.file_uploader("üìÑ Envie o PDF do Lattes", type=["pdf"], accept_multiple_files=False)

use_llm = st.checkbox("Usar IA (LLM) para estruturar a lista de artigos (mais robusto)", value=True)

if uploaded:
    st.session_state.messages.append({"role": "user", "content": f"Enviei o arquivo: **{uploaded.name}**"})
    with st.chat_message("user"):
        st.markdown(f"Enviei o arquivo: **{uploaded.name}**")

    file_bytes = uploaded.read()

    with st.chat_message("assistant"):
        st.markdown("‚úÖ Recebido! Lendo o PDF e localizando a se√ß√£o de **Artigos completos publicados em peri√≥dicos**‚Ä¶")

    text = pdf_to_text(file_bytes)
    section = slice_journal_section(text)

    if not section:
        with st.chat_message("assistant"):
            st.error("N√£o encontrei a se√ß√£o de artigos em peri√≥dicos. Se o PDF estiver escaneado (imagem), ser√° necess√°rio OCR.")
        st.stop()

    # Extrai
    if use_llm:
        df = llm_extract_articles(section)
    else:
        df = extract_articles_heuristic(section)

    if df.empty:
        with st.chat_message("assistant"):
            st.error("N√£o consegui extrair itens v√°lidos. Tente habilitar o modo IA (LLM) ou use OCR se o PDF for imagem.")
        st.stop()

    with st.chat_message("assistant"):
        st.success(f"Encontrei **{len(df)}** artigos. Agora, por favor, **confirme/edite** ano, t√≠tulo e DOI.")

    st.subheader("1) Confirma√ß√£o dos artigos extra√≠dos")
    df_edit = st.data_editor(
        df[["ano", "titulo", "doi"]].copy(),
        num_rows="dynamic",
        use_container_width=True
    )

    st.subheader("2) Cita√ß√µes (OpenAlex) e gr√°ficos")
    fetch_cit = st.checkbox("Buscar cita√ß√µes no OpenAlex (por DOI)", value=True)

    if st.button("Gerar gr√°ficos"):
        df_final = df_edit.copy()
        df_final["ano"] = pd.to_numeric(df_final["ano"], errors="coerce").astype("Int64")
        df_final["titulo"] = df_final["titulo"].fillna("").astype(str).str.strip()
        df_final["doi"] = df_final["doi"].fillna("").astype(str).str.strip()
        df_final = df_final[df_final["titulo"].str.len() > 3].reset_index(drop=True)

        if df_final.empty:
            st.error("Ap√≥s a edi√ß√£o, n√£o restaram itens v√°lidos.")
            st.stop()

        if fetch_cit:
            prog = st.progress(0)
            status = st.empty()

            def progress_cb(done, total):
                prog.progress(int(done / total * 100))
                status.text(f"Buscando cita√ß√µes: {done}/{total}")

            df_final = add_citations(df_final, progress_cb=progress_cb)
            status.text("Cita√ß√µes coletadas (quando DOI foi encontrado no OpenAlex).")
            prog.progress(100)
        else:
            df_final["citacoes"] = pd.NA

        # Gr√°fico 1: publica√ß√µes por ano
        pub_by_year = (
            df_final.dropna(subset=["ano"])
            .groupby("ano", as_index=False)
            .size()
            .rename(columns={"size": "publicacoes"})
            .sort_values("ano")
        )

        # Gr√°fico 2: m√©dia de cita√ß√µes por ano (ano = ano de publica√ß√£o)
        cit_by_year = (
            df_final.dropna(subset=["ano"])
            .assign(citacoes=pd.to_numeric(df_final["citacoes"], errors="coerce"))
            .dropna(subset=["citacoes"])
            .groupby("ano", as_index=False)["citacoes"]
            .mean()
            .rename(columns={"citacoes": "media_citacoes"})
            .sort_values("ano")
        )

        c1, c2 = st.columns(2, gap="large")

        with c1:
            st.markdown("### üìà Quantidade de publica√ß√µes por ano")
            fig1 = px.bar(pub_by_year, x="ano", y="publicacoes", text="publicacoes")
            fig1.update_layout(xaxis_title="Ano", yaxis_title="Publica√ß√µes", showlegend=False)
            st.plotly_chart(fig1, use_container_width=True)

        with c2:
            st.markdown("### üìä M√©dia de cita√ß√µes por ano (por ano de publica√ß√£o)")
            if cit_by_year.empty:
                st.warning("Sem dados suficientes de cita√ß√µes (provavelmente faltam DOIs ou o OpenAlex n√£o encontrou os trabalhos).")
            else:
                fig2 = px.line(cit_by_year, x="ano", y="media_citacoes", markers=True)
                fig2.update_layout(xaxis_title="Ano", yaxis_title="M√©dia de cita√ß√µes", showlegend=False)
                st.plotly_chart(fig2, use_container_width=True)

        st.markdown("### üîé Dados finais")
        st.dataframe(df_final, use_container_width=True)
        st.download_button(
            "Baixar CSV",
            df_final.to_csv(index=False).encode("utf-8"),
            file_name="artigos_lattes.csv",
            mime="text/csv"
        )
